{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been exported successfully.\n",
      "wiki_movies is created.\n",
      "wiki_movies_df is created.\n",
      "wiki_movies_df is cleaned.\n",
      "wiki_movies_df is updated.\n",
      "step1 is done\n",
      "step2 is done\n",
      "step3 is done\n",
      "step4 is done\n",
      "Files have been transformed successfully.\n",
      "6051 Record deleted successfully from movies\n",
      "COMPLETE. Record deleted successfully from movies\n",
      "26024289 Record deleted successfully from ratings\n",
      "COMPLETE. Record deleted successfully from ratings\n",
      "importing rows 0 to 1000000..."
     ]
    }
   ],
   "source": [
    "#Import all dependencies\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from config import db_password\n",
    "import time\n",
    "\n",
    "#Create a file directory path\n",
    "file_dir = '/Users/trahin/Desktop/Data_Bootcamp/1_repo/Module8-Challenge-JB-Trahin'\n",
    "\n",
    "#Declaring new files to upload\n",
    "file_1 = 'wikipedia-movies.json'\n",
    "file_2 = 'movies_metadata.csv'\n",
    "file_3 = 'ratings.csv'\n",
    "\n",
    "#Create a function that takes in three arguments and perform all ETL steps\n",
    "def new_files_to_load(wiki_file,kaggle_file,ratings):\n",
    "    #Call new_files_to_load function\n",
    "    with open(f'{file_dir}/{wiki_file}', mode='r') as file:\n",
    "        wiki_movies_raw = json.load(file)\n",
    "    kaggle_metadata = pd.read_csv(f'{file_dir}/{kaggle_file}', low_memory=False)\n",
    "    ratings = pd.read_csv(f'{file_dir}/{ratings}', low_memory=False)\n",
    "    print('Files have been exported successfully.')   \n",
    "    #Step1: Cleaning Wikipedia data\n",
    "    \n",
    "    #Select all movies with data in Director and Directed by columns, and exclude series with episodes\n",
    "    wiki_movies = [movie for movie in wiki_movies_raw if ('Director' in movie or 'Directed by' in movie)\n",
    "                   and 'imdb_link' in movie\n",
    "                   and 'No. of episodes' not in movie]\n",
    "    print('wiki_movies is created.') \n",
    "    \n",
    "    # Create a DataFrame for movies\n",
    "    wiki_movies_df = pd.DataFrame(wiki_movies)\n",
    "    print('wiki_movies_df is created.') \n",
    "    \n",
    "    #Create a function to clean wiki_movies_raw\n",
    "    def clean_movie(movie):\n",
    "        movie = dict(movie) #create a non-destructive copy\n",
    "        alt_titles = {}\n",
    "        # combine alternate titles into one list\n",
    "        for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                    'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                    'Mandarin','McCune-Reischauer','Original title','Polish',\n",
    "                    'Revised Romanization','Romanized','Russian',\n",
    "                    'Simplified','Traditional','Yiddish']:\n",
    "            if key in movie:\n",
    "                alt_titles[key] = movie[key]\n",
    "                movie.pop(key)\n",
    "        if len(alt_titles) > 0:\n",
    "            movie['alt_titles'] = alt_titles\n",
    "\n",
    "        # merge column names\n",
    "        def change_column_name(old_name, new_name):\n",
    "            if old_name in movie:\n",
    "                movie[new_name] = movie.pop(old_name)\n",
    "        change_column_name('Adaptation by', 'Writer(s)')\n",
    "        change_column_name('Country of origin', 'Country')\n",
    "        change_column_name('Directed by', 'Director')\n",
    "        change_column_name('Distributed by', 'Distributor')\n",
    "        change_column_name('Edited by', 'Editor(s)')\n",
    "        change_column_name('Length', 'Running time')\n",
    "        change_column_name('Original release', 'Release date')\n",
    "        change_column_name('Music by', 'Composer(s)')\n",
    "        change_column_name('Produced by', 'Producer(s)')\n",
    "        change_column_name('Producer', 'Producer(s)')\n",
    "        change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "        change_column_name('Productioncompany ', 'Production company(s)')\n",
    "        change_column_name('Released', 'Release Date')\n",
    "        change_column_name('Release Date', 'Release date')\n",
    "        change_column_name('Screen story by', 'Writer(s)')\n",
    "        change_column_name('Screenplay by', 'Writer(s)')\n",
    "        change_column_name('Story by', 'Writer(s)')\n",
    "        change_column_name('Theme music composer', 'Composer(s)')\n",
    "        change_column_name('Written by', 'Writer(s)')\n",
    "\n",
    "        return movie\n",
    "\n",
    "    #Create a list of clean movies and create a dataframe\n",
    "    clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "    wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "    print('wiki_movies_df is cleaned.')\n",
    "        \n",
    "    #Get IMDb IDs from IMDb links and drop  duplicates\n",
    "    wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "    wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "    \n",
    "    #Find and keep columns with less than 90% null values and update dataframe\n",
    "    wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "    wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "    print('wiki_movies_df is updated.')   \n",
    "        \n",
    "    #Drop rows with missing values in columns\n",
    "    box_office = wiki_movies_df['Box office'].dropna()\n",
    "    budget = wiki_movies_df['Budget'].dropna()\n",
    "        \n",
    "    #Concatenate all list items into one string\n",
    "    box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "        \n",
    "    #Create a variable form_one and set it equal to the finished regular expression string\n",
    "    form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "    #Create another variable form_two and set it equal to the finished regular expression string\n",
    "    form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "        \n",
    "    #date forms\n",
    "    #1-Full month name, one- to two-digit day, four-digit year (i.e., January 1, 2000)\n",
    "    date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]\\d,\\s\\d{4}'\n",
    "    #2-Four-digit year, two-digit month, two-digit day, with any separator (i.e., 2000-01-01)\n",
    "    date_form_two = r'\\d{4}.[01]\\d.[123]\\d'\n",
    "    #3-Full month name, four-digit year (i.e., January 2000)\n",
    "    date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "    #4-Four-digit year\n",
    "    date_form_four = r'\\d{4}'\n",
    "        \n",
    "    #Search for any string that starts with a dollar sign and ends with a hyphen, and then replace it with just a dollar sign\n",
    "    box_office = box_office.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "    running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    #Convert strings to numeric values.\n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "        \n",
    "    #Create a function that turns the extracted values into a numeric value.\n",
    "    def parse_dollars(s):\n",
    "        # if s is not a string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "\n",
    "        # if input is of the form $###.# million\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "            # remove dollar sign and \" million\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "            # convert to float and multiply by a million\n",
    "            value = float(s) * 10**6\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###.# billion\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "            # remove dollar sign and \" billion\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "            # convert to float and multiply by a billion\n",
    "            value = float(s) * 10**9\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###,###,###\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "            # remove dollar sign and commas\n",
    "            s = re.sub('\\$|,','', s)\n",
    "            # convert to float\n",
    "            value = float(s)\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # otherwise, return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "        \n",
    "    #Extract values and apply parse_dollars to the first column in the DataFrame returned by str.extract\n",
    "    wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "    wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "       \n",
    "    #Drop the old Box Office column\n",
    "    wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "    wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "    wiki_movies_df.drop('Release date', axis=1, inplace=True)\n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "    print('step1 is done')\n",
    "    \n",
    "    #Step2: Cleaning Kaggle data\n",
    "    #Keep rows where the adult column is False, and then drop the adult column\n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "        \n",
    "    #Create a Boolean column and assign it back to video\n",
    "    kaggle_metadata['video'] = kaggle_metadata['video'] == 'True'\n",
    "        \n",
    "    #Covert to numeric columns\n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "        \n",
    "    #Convert release_date to datetime\n",
    "    kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "        \n",
    "    #Convert timestamp to datetime\n",
    "    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "    print('step2 is done') \n",
    "    \n",
    "    #Step3: Merge Wikipedia and Kaggle Data\n",
    "    #Identify columns that are redundant across two datasets\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "        \n",
    "    #Drop rows where data is corrupted because two movies got merged\n",
    "    try:\n",
    "        movies_df = movies_df.drop(movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index)\n",
    "    except:\n",
    "        print('No rows to drop because of data corruption')\n",
    "            \n",
    "    #Drop the title_wiki, release_date_wiki, Language, and Production company(s) columns\n",
    "    movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "\n",
    "    #Create a function that fills in missing data for a column pair and then drops the redundant column\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(\n",
    "            lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column], axis=1)\n",
    "        df.drop(columns=wiki_column, inplace=True)\n",
    "          \n",
    "    #3-Run the function for the three column pairs that we decided to fill in zeros\n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "        \n",
    "    #Check that there aren’t any columns with only one value.\n",
    "    try:\n",
    "        for col in movies_df.columns:\n",
    "            lists_to_tuples = lambda x: tuple(x) if type(x) == list else x\n",
    "            value_counts = movies_df[col].apply(lists_to_tuples).value_counts(dropna=False)\n",
    "            num_values = len(value_counts)\n",
    "            if num_values == 1:\n",
    "                #drop the column as we don't need it\n",
    "                movies_df.drop(col, axis=1, inplace=True)\n",
    "    except:\n",
    "        print('No columns to drop because of only one value')\n",
    "        \n",
    "    #Reorder the columns\n",
    "    movies_df = movies_df[['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                       'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                       'genres','original_language','overview','spoken_languages','Country',\n",
    "                       'production_companies','production_countries','Distributor',\n",
    "                       'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                      ]]\n",
    "        \n",
    "    #Rename the columns\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                  'title_kaggle':'title',\n",
    "                  'url':'wikipedia_url',\n",
    "                  'budget_kaggle':'budget',\n",
    "                  'release_date_kaggle':'release_date',\n",
    "                  'Country':'country',\n",
    "                  'Distributor':'distributor',\n",
    "                  'Producer(s)':'producers',\n",
    "                  'Director':'director',\n",
    "                  'Starring':'starring',\n",
    "                  'Cinematography':'cinematography',\n",
    "                  'Editor(s)':'editors',\n",
    "                  'Writer(s)':'writers',\n",
    "                  'Composer(s)':'composers',\n",
    "                  'Based on':'based_on'\n",
    "                 }, axis='columns', inplace=True)\n",
    "    print('step3 is done')     \n",
    "        \n",
    "    #Step4: Transform and merge ratings data\n",
    "        \n",
    "    #Count how many times a movie received a given rating:\n",
    "    #Use a groupby on the “movieId” and “rating” columns and take the count for each group\n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count()\n",
    "\n",
    "    #Rename the “userId” column to “count.”\n",
    "    rating_counts = rating_counts.rename({'userId':'count'}, axis=1) \n",
    "\n",
    "    #Pivot this data so that movieId is the index, the columns will be all the rating values, and the rows will be the counts for each rating value.\n",
    "    rating_counts = rating_counts.pivot(index='movieId',columns='rating', values='count')\n",
    "\n",
    "    #Rename the columns so they’re easier to understand.\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "        \n",
    "    #Merge the rating counts in movies_df:\n",
    "    #Use a left merge, since we want to keep everything in movies_df.\n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "\n",
    "    #Fill in missing values\n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "    print('step4 is done')    \n",
    "    print('Files have been transformed successfully.')\n",
    "        \n",
    "    #Step5: Load and update data into SQL tables\n",
    "        \n",
    "    #Establish connection string:\n",
    "    db_string = f\"postgres://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "\n",
    "    #Create the database engine\n",
    "    engine = create_engine(db_string)\n",
    "        \n",
    "    #Delete data from movies table but keep tables\n",
    "    try:\n",
    "        connection = psycopg2.connect(db_string)\n",
    "        cursor = connection.cursor()\n",
    "        sql_delete_query = \"DELETE FROM movies\"\n",
    "        cursor.execute(sql_delete_query)\n",
    "        connection.commit()\n",
    "        count = cursor.rowcount\n",
    "        print(count, \"Record deleted successfully from movies\")\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Record from movies deleted. Complete.\")\n",
    "    \n",
    "    print(\"COMPLETE. Record deleted successfully from movies\")    \n",
    "    \n",
    "    #Delete data from ratings table but keep tables\n",
    "    try:\n",
    "        connection = psycopg2.connect(db_string)\n",
    "        cursor = connection.cursor()\n",
    "        sql_delete_query = \"DELETE FROM ratings\"\n",
    "        cursor.execute(sql_delete_query)\n",
    "        connection.commit()\n",
    "        count = cursor.rowcount\n",
    "        print(count, \"Record deleted successfully from ratings\")\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error in Delete operation\", error)\n",
    "            \n",
    "    print(\"COMPLETE. Record deleted successfully from ratings\")   \n",
    "    \n",
    "    #Import the movie data\n",
    "    movies_df.to_sql(name='movies', con=engine, if_exists='append')\n",
    "        \n",
    "    #Import the ratings data\n",
    "    # create a variable for the number of rows imported\n",
    "    rows_imported = 0\n",
    "    # get the start_time from time.time()\n",
    "    start_time = time.time()\n",
    "    for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "        # print out the range of rows that are being imported\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "            \n",
    "        data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "\n",
    "        # increment the number of rows imported by the size of 'data'\n",
    "        rows_imported += len(data)\n",
    "\n",
    "        # add elapsed time to final print out\n",
    "        print(f'Done. {time.time() - start_time} total seconds elapsed')\n",
    "\n",
    "    print('step5 is done') \n",
    "    print('Files have been updated and loaded successfully.')\n",
    "\n",
    "#Call the function\n",
    "new_files_to_load(file_1,file_2,file_3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
